{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPpmJXBgXoKmxBdC06+2ool",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JXqwq/dl_lecture_VQA/blob/main/FinalProject_VQA_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import time\n",
        "from statistics import mode\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from transformers import BertModel, BertTokenizer"
      ],
      "metadata": {
        "id": "mLU9ONwTzzJv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xKs5oQXPbZ6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o \"/content/drive/My Drive/VQA/train.zip\" -d \"/content/drive/My Drive/data/train\""
      ],
      "metadata": {
        "id": "c6tvudgrZs9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o \"/content/drive/My Drive/VQA/valid.zip\" -d \"/content/drive/My Drive/data/valid\""
      ],
      "metadata": {
        "id": "8sNRX1VrvjWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vn2uVNZXziiU"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def process_text(text):\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 数詞を数字に変換\n",
        "    num_word_to_digit = {\n",
        "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
        "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
        "        'ten': '10'\n",
        "    }\n",
        "    for word, digit in num_word_to_digit.items():\n",
        "        text = text.replace(word, digit)\n",
        "\n",
        "    # 小数点のピリオドを削除\n",
        "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
        "\n",
        "    # 冠詞の削除\n",
        "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
        "\n",
        "    # 短縮形のカンマの追加\n",
        "    contractions = {\n",
        "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
        "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n",
        "    }\n",
        "    for contraction, correct in contractions.items():\n",
        "        text = text.replace(contraction, correct)\n",
        "\n",
        "    # 句読点をスペースに変換\n",
        "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
        "\n",
        "    # 句読点をスペースに変換\n",
        "    text = re.sub(r'\\s+,', ',', text)\n",
        "\n",
        "    # 連続するスペースを1つに変換\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# 1. データローダーの作成\n",
        "class VQADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df_path, image_dir, transform=None, answer=True):\n",
        "        self.transform = transform  # 画像の前処理\n",
        "        self.image_dir = image_dir  # 画像ファイルのディレクトリ\n",
        "        self.df = pandas.read_json(df_path)  # 画像ファイルのパス，question, answerを持つDataFrame\n",
        "        self.answer = answer\n",
        "\n",
        "        # question / answerの辞書を作成\n",
        "        #self.question2idx = {}\n",
        "        self.answer2idx = {}\n",
        "        #self.idx2question = {}\n",
        "        self.idx2answer = {}\n",
        "\n",
        "        # 質問文に含まれる単語を辞書に追加\n",
        "        #for question in self.df[\"question\"]:\n",
        "        #   question = process_text(question)\n",
        "        #    words = question.split(\" \")\n",
        "        #    for word in words:\n",
        "        #        if word not in self.question2idx:\n",
        "        #            self.question2idx[word] = len(self.question2idx)\n",
        "        #self.idx2question = {v: k for k, v in self.question2idx.items()}  # 逆変換用の辞書(question)\n",
        "\n",
        "        if self.answer:\n",
        "            # 回答に含まれる単語を辞書に追加\n",
        "            for answers in self.df[\"answers\"]:\n",
        "                for answer in answers:\n",
        "                    word = answer[\"answer\"]\n",
        "                    word = process_text(word)\n",
        "                    if word not in self.answer2idx:\n",
        "                        self.answer2idx[word] = len(self.answer2idx)\n",
        "            self.idx2answer = {v: k for k, v in self.answer2idx.items()}  # 逆変換用の辞書(answer)\n",
        "\n",
        "    def update_dict(self, dataset):\n",
        "        \"\"\"\n",
        "        検証用データ，テストデータの辞書を訓練データの辞書に更新する．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset : Dataset\n",
        "            訓練データのDataset\n",
        "        \"\"\"\n",
        "        #self.question2idx = dataset.question2idx\n",
        "        self.answer2idx = dataset.answer2idx\n",
        "        #self.idx2question = dataset.idx2question\n",
        "        self.idx2answer = dataset.idx2answer\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        対応するidxのデータ（画像，質問，回答）を取得．\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : int\n",
        "            取得するデータのインデックス\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        image : torch.Tensor  (C, H, W)\n",
        "            画像データ\n",
        "        question : torch.Tensor  (vocab_size)\n",
        "            質問文をone-hot表現に変換したもの\n",
        "        answers : torch.Tensor  (n_answer)\n",
        "            10人の回答者の回答のid\n",
        "        mode_answer_idx : torch.Tensor  (1)\n",
        "            10人の回答者の回答の中で最頻値の回答のid\n",
        "        \"\"\"\n",
        "        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        #question = np.zeros(len(self.idx2question) + 1)  # 未知語用の要素を追加\n",
        "        #question_words = self.df[\"question\"][idx].split(\" \")\n",
        "        #for word in question_words:\n",
        "        #    try:\n",
        "        #        question[self.question2idx[word]] = 1  # one-hot表現に変換\n",
        "        #    except KeyError:\n",
        "        #        question[-1] = 1  # 未知語\n",
        "        question = self.df[\"question\"][idx]\n",
        "\n",
        "        if self.answer:\n",
        "            answers = [self.answer2idx[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\n",
        "            mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）\n",
        "\n",
        "            return image, question, torch.Tensor(answers), int(mode_answer_idx)\n",
        "\n",
        "        else:\n",
        "            return image, question\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 評価指標の実装\n",
        "# 簡単にするならBCEを利用する\n",
        "def VQA_criterion(batch_pred: torch.Tensor, batch_answers: torch.Tensor):\n",
        "    total_acc = 0.\n",
        "\n",
        "    for pred, answers in zip(batch_pred, batch_answers):\n",
        "        acc = 0.\n",
        "        for i in range(len(answers)):\n",
        "            num_match = 0\n",
        "            for j in range(len(answers)):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if pred == answers[j]:\n",
        "                    num_match += 1\n",
        "            acc += min(num_match / 3, 1)\n",
        "        total_acc += acc / 10\n",
        "\n",
        "    return total_acc / len(batch_pred)"
      ],
      "metadata": {
        "id": "y57EM1Rl0AYD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. モデルのの実装\n",
        "# ResNetを利用できるようにしておく\n",
        "# Modified: Use pretrained ResNet models & BERT for text preprocessing\n",
        "# Reference: https://farooqsk.medium.com/introduction-to-visual-question-answering-in-pytorch-7b5cc61c86d, https://towardsdatascience.com/feature-extraction-with-bert-for-text-classification-533dde44dc2f\n",
        "\n",
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, n_answer: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # Fine tune the pretrained ResNet50Image Network\n",
        "        self.resnet = torchvision.models.resnet50(pretrained=True)\n",
        "        num_ftrs = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "          nn.Dropout(0.5),\n",
        "          nn.Linear(num_ftrs, 512),\n",
        "          nn.Tanh(),\n",
        "          nn.Linear(512, 128),\n",
        "          nn.Tanh(),\n",
        "          nn.Linear(128, 32)\n",
        "        )\n",
        "\n",
        "        # Question Network\n",
        "        self.text_encoder = BertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "\n",
        "        # Merge image and question output\n",
        "        self.fc2 = nn.Linear(800, 400) # image features + BERT features = 800\n",
        "        self.fc3 = nn.Linear(400, n_answer)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        #self.fc = nn.Sequential(\n",
        "        #    nn.Linear(1024, 512),\n",
        "        #    nn.ReLU(inplace=True),\n",
        "        #    nn.Linear(512, n_answer)\n",
        "        #)\n",
        "\n",
        "    def forward(self, image, question):\n",
        "        image_feature = self.resnet(image)  # 画像の特徴量\n",
        "\n",
        "        question_tokens = self.tokenizer(question, padding = True, truncation = True, return_tensors=\"pt\")\n",
        "        question_tokens = {k: v.to(image.device) for k, v in question_tokens.items()}\n",
        "        outputs = self.text_encoder(**question_tokens)\n",
        "        question_feature = outputs.last_hidden_state[:,0,:] # テキストの特徴量\n",
        "\n",
        "        act = nn.ReLU(inplace=True)\n",
        "\n",
        "        x = torch.cat([image_feature, question_feature], dim=1)\n",
        "        x = self.fc2(x)\n",
        "        x = act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "g_hJfq8e0Dc7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 学習の実装\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    simple_acc = 0\n",
        "\n",
        "    start = time.time()\n",
        "    for image, question, answers, mode_answer in dataloader:\n",
        "        image, answer, mode_answer = \\\n",
        "            image.to(device), answers.to(device), mode_answer.to(device)\n",
        "\n",
        "        pred = model(image, question)\n",
        "        loss = criterion(pred, mode_answer.squeeze())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n",
        "        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy\n",
        "\n",
        "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\n",
        "\n",
        "\n",
        "def eval(model, dataloader, optimizer, criterion, device):\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    simple_acc = 0\n",
        "\n",
        "    start = time.time()\n",
        "    for image, question, answers, mode_answer in dataloader:\n",
        "        image, answer, mode_answer = \\\n",
        "            image.to(device), answers.to(device), mode_answer.to(device)\n",
        "\n",
        "        pred = model(image, question)\n",
        "        loss = criterion(pred, mode_answer.squeeze())\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n",
        "        simple_acc += (pred.argmax(1) == mode_answer).mean().item()  # simple accuracy\n",
        "\n",
        "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start"
      ],
      "metadata": {
        "id": "SuZPvvfW6_uF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deviceの設定\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# dataloader / model\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # Data augmentation: transforms.RandomHorizontalFlip()\n",
        "    transforms.ToTensor()\n",
        "      ])\n",
        "\n",
        "\n",
        "# For colab\n",
        "train_dataset = VQADataset(df_path=\"/content/drive/My Drive/VQA/train.json\", image_dir=\"/content/drive/My Drive/data/train/train\", transform=transform)\n",
        "test_dataset = VQADataset(df_path=\"/content/drive/My Drive/VQA/valid.json\", image_dir=\"/content/drive/My Drive/data/valid/valid\", transform=transform, answer=False)\n",
        "test_dataset.update_dict(train_dataset)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "model = VQAModel(n_answer=len(train_dataset.answer2idx)).to(device)\n"
      ],
      "metadata": {
        "id": "gywxZarb7DVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer / criterion\n",
        "num_epoch = 5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "# train model\n",
        "for epoch in range(num_epoch):\n",
        "    train_loss, train_acc, train_simple_acc, train_time = train(model, train_loader, optimizer, criterion, device)\n",
        "    print(f\"【{epoch + 1}/{num_epoch}】\\n\"\n",
        "          f\"train time: {train_time:.2f} [s]\\n\"\n",
        "          f\"train loss: {train_loss:.4f}\\n\"\n",
        "          f\"train acc: {train_acc:.4f}\\n\"\n",
        "          f\"train simple acc: {train_simple_acc:.4f}\")\n",
        "    path_name = f\"/content/drive/My Drive/data/VQA_epoch{epoch + 1}.pth\"\n",
        "    torch.save(model, path_name)"
      ],
      "metadata": {
        "id": "b3FO6qWLw9bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(\"/content/drive/My Drive/data/VQA_epoch4.pth\")\n",
        "# 提出用ファイルの作成\n",
        "model.eval()\n",
        "submission = []\n",
        "for image, question in test_loader:\n",
        "    image, question = image.to(device), question\n",
        "    pred = model(image, question)\n",
        "    pred = pred.argmax(1).cpu().item()\n",
        "    submission.append(pred)\n",
        "\n",
        "submission = [train_dataset.idx2answer[id] for id in submission]\n",
        "submission = np.array(submission)\n",
        "#torch.save(model.state_dict(), \"model.pth\")\n",
        "np.save(\"/content/drive/My Drive/submission.npy\", submission)"
      ],
      "metadata": {
        "id": "udJ3ZLC6BQWX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}